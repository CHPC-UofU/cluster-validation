#!/bin/bash
#SBATCH -J HPL_test_ash # A single job name for the array 
##SBATCH -N 58 # All tasks on one machine 
#SBATCH --ntasks-per-node=20
#SBATCH -p ash-guest
#SBATCH -A smithp-guest
#SBATCH -o ash-%j.out # name of the stdout
#SBATCH -e ash-%j.err # name of the stderr
##SBATCH --reservation=downtime-2022-12-06
#SBATCH -t 0-8:00 # 2 hours (D-HH:MM) 

# this script runs HPL on all ash nodes that are available
# find out how many nodes are available and put it to the -N flag
# the script should figure out the rest to run HPL on 32 tasks per node 

module purge
module load gcc/8.5.0  openmpi/4.1.3 hpl

MPI_PROC_NUM=$SLURM_NTASKS

# Memory per node in GB
if [ ! -v MEM ]; then
  MEM=$(free -g | awk '/Mem:/ {print $2}')
fi

# memory size per node
MEM=64
# % of memory size for the run
SIZE=50
NB=192

words=$((SLURM_JOB_NUM_NODES*MEM*1024*1024*1024/8))

if [ ! -v P ]; then
P=$(echo "scale=0;sqrt($MPI_PROC_NUM)" | bc)
Q=$((MPI_PROC_NUM/P))
# Find P and Q
while [ $((P*Q)) -ne $MPI_PROC_NUM ]; do
  P=$((P-1))
  Q=$((MPI_PROC_NUM/P))
done
fi

# N should be a multiple of NB and least common multiple of P and Q
A=$P
B=$Q
R=1

# Find greatest common divisor
while [ $R -ne 0 ]; do
  if [ $A -gt $B ]; then
    R=$(( A % B ))
    if [ $R -ne 0 ]; then A=$R;
    else GCD=$B; fi
  else
    R=$(( B % A ))
    if [ $R -ne 0 ]; then B=$R;
    else GCD=$A; fi
  fi
done

# least common multiple
LCM=$(( P * Q / GCD ))

N=$(echo "scale=0;sqrt($words*$SIZE/100)/($LCM*$NB)" | bc)
N=$((N*LCM*NB))

cp HPL.dat.tpl HPL.dat
sed "s/%NB%/$NB/" -i HPL.dat
sed "s/%N%/$N/" -i HPL.dat
sed "s/%P%/$P/" -i HPL.dat
sed "s/%Q%/$Q/" -i HPL.dat

mpirun -n $SLURM_NTASKS xhpl 

